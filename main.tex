% ОБЯЗАТЕЛЬНО ИМЕННО ТАКОЙ documentclass!
% (Основной кегль = 14pt, поэтому необходим extsizes)
% Формат, разумеется, А4
% article потому что стандарт не подразумевает разделов
% Глава = section, Параграф = subsection
% (понятия "глава" и "параграф" из стандарта)
\documentclass[a4paper,article,14pt]{extarticle}

% Подключаем главный пакет со всем необходимым
\usepackage{spbudiploma_tempora}

% Пакеты по желанию (самые распространенные)
% Хитрые мат. символы
\usepackage{euscript}
% Таблицы
\usepackage{longtable}
\usepackage{makecell}
% Картинки (можно встявлять даже pdf)
\usepackage[pdftex]{graphicx}

\usepackage{amsthm,amssymb, amsmath}

% доп символы
\usepackage{textcomp}
\usepackage{dsfont}

% оформление алгоритма
%\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algorithm,algpseudocode}
\usepackage{float}
\usepackage{lipsum}


\makeatletter
\newenvironment{breakablealgorithm}
  {% \begin{breakablealgorithm}
   \begin{center}
     \refstepcounter{algorithm}% New algorithm
     \hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
     \renewcommand{\caption}[2][\relax]{% Make a new \caption
       {\raggedright\textbf{\fname@algorithm~\thealgorithm} ##2\par}%
       \ifx\relax##1\relax % #1 is \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
       \else % #1 is not \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
       \fi
       \kern2pt\hrule\kern2pt
     }
  }{% \end{breakablealgorithm}
     \kern2pt\hrule\relax% \@fs@post for \@fs@ruled
   \end{center}
  }
\makeatother


\begin{document}

% Титульник в файле titlepage.tex
\input{titlepage.tex}

% Содержание
\tableofcontents
\pagebreak

\specialsection{Введение}
С каждым годом растет количество
накопленных человечеством данных. Современное аппаратное обеспечение часто не позволяет проводить вычисления
с большим количеством данных на одной машине. Это часто связано во-первых, с ограниченным объемом оперативной памяти компьютера,
а во-вторых, с ограниченным временным на решение конкретной задачи. Область машинного обучения испытывает обе перечисленные проблемы.
Часто для достижения требуемого результата необходимо обучение нейронной сети с множеством параметров на огромном объеме данных.
Это препятствует прогрессу в исследованиях и разработках.  В связи с этим существует потребность в методах распределенного обучения.
Они позволяют не ограничиваться мощностями одной машины при конструировании новых методов в машинном обучении
и ускоряют уже существующие решения.

Помимо этого на практике часто возникает потребность обрабатывать информацию разной степени приватности. В некоторых случаях
при обучении нейронной сети используются данные, распространение которых запрещено или нежелательно. Отличным примером этого
является какой-либо совместный проект двух банков, в ходе которого они хотят решить общую задачу, но не могут передать, пусть даже
и обезличенные, данные своих клиентов друг другу. В этом случае возникает необходимость использовать методы распределенного обучения, которые
обеспечивают изоляцию данных.

Топология сети и методы взаимодействия узлов являются важными свойствами распределенных систем. Не всегда вычислительные
кластеры имеют регулярную структуру, часто доступные вычислительные ресурсы представляют собой гетерогенную компьютерную сеть,
где пропускная способность каналов связи и мощности каждого узла могут сильно отличаться. В этом случае применяемые методы распределенного
обучения должны быть устойчивы к неоднородности сети.

В данной работе проводится краткий обзор основных методов распределенного обучения нейронных сетей и исследование возможности применения алгоритма на базе консенсуса и алгоритма сплетен для распределенного обучения с изоляцией данных. Также рассматривается
способность исследуемого алгоритма корректно решать задачу в сети с неоднородной вычислительной мощностью узлов.
\pagebreak

\specialsection{Постановка задачи}
Целью данной работы является исследование возможности применения алгоритма на базе консенсуса и алгоритма сплетен для распределенного
обучения нейронных сетей. Для достижения поставленной цели были сформулированны следующие задачи:

\begin{itemize}
  \item изучение предметной области и обзор существующих методов
  \item изучение математической теории, описывающей исследуемый подход
  \item реализация алгоритма и прототипа распределенной сети для проведения экспериментов
  \item проведение экспериментов и анализ результатов
\end{itemize}
\pagebreak

\section{Теория}
\subsection{Задача машинного обучения}
Вспомним стандартную постановку задачи машинного обучения c учителем: есть некоторое параметрическое семейство функций  $f(\bullet, \theta)$ и набор данных $(x_i, y_i)_{i=0}^{m-1}$, требуется подобрать параметры $\theta$ так, чтобы $f(\bullet, \theta)$ как можно лучше соответствовала этим данным.

Формулировка в виде задачи оптимизации:
 \begin{equation} \label{eq:ml_task}
 \mathcal{J}(\theta)=
 \frac{1}{m}\sum_{i=0}^{m-1}g(f(x_i, \theta), y_i)\rightarrow_{\theta}\min
 \end{equation}
где $g(f(x_i, \theta), y_i)$ -- функция ошибки.

Часто эту задачу решают с помощью пакетного градиентного спуска (Mini Batch Gradient Descent) \cite{mbgd}:
  \begin{equation} \label{eq:mbgd}
\theta_{k+1} = \theta_k - \alpha_k\frac{1}{|S_k|}\sum_{i\in S_k}\nabla g(f(x_i, \theta_k), y_i)
 \end{equation}
где $S_k$ –- случайно выбранное подмножество $\{0, \ldots, m-1\}$, $\alpha_k$ –- коэффициент скорости обучения.

\subsection{Алгоритм среднего консенсуса}
\label{section:algo_mean_cons}

Алгоритм консенсуса -- это процесс в теории автоматического управления, используемый для достижения согласия по единому значению данных среди распределенных процессов или систем. То есть форма стабильной системы, в которой состояния $\pi(t)_i$ распределенной системы сошлись к одному значению:
\begin{equation}
\pi_i(t)\rightarrow \pi^*,~0\leq i\leq m-1.
\end{equation}

Базовый (средний) консенсус можно использовать для децентрализованного вычисления среднего нескольких чисел, каждое из которых хранится на отдельном вычислительном узле \cite{consensus_basics}:
\begin{equation}
x^0 = (x_1^0, \ldots, x_p^0)
\end{equation}
\begin{equation}
x_i^* = \frac{1}{p}\sum_{j=1}^{p}x_j^0.
\end{equation}

Рассмотрим сеть агентов, принимающих решения, с динамикой $\dot x_i = u_i$, заинтересованных в достижении консенсуса посредством локальной коммуникации со своими соседями на графе $G = (V, E)$.
То есть сходимости:
\begin{equation}
x_1 = x_2 = \ldots = x_n.
\end{equation}

Это может быть выражено как $x = \alpha \mathds{1} $, где $\mathds{1} = (1, \ldots, 1)^T$ и $\alpha \in R$ -- коллективное решение группы агентов. Пусть $A = a_{ij}$ -- матрица смежности графа $G$. Множество соседей агента $i$ -- это $N_i$ и определяется формулой
\begin{equation}
N_i = \{ j \in V: a_{ij} \ne 0 \}; \quad \quad V = \{1, \ldots, n\}.
\end{equation}

Агент $i$ коммуницирует с агентом $j$, если $j$ является соседом $i$. Набор всех узлов и их соседей определяет набор ребер графа
\begin{equation}
E = \{ (i,j) \in V\times V: a_{ij} \ne 0\}.
\end{equation}

Динамический граф $G(t) = (V, E(t))$ -- это граф, в котором набор ребер $E(t)$ и матрица смежности $A(t)$ изменяются во времени. Ясно, что множество соседей $N_i(t)$ каждого агента в динамическом графе также является изменяющимся во времени множеством.

В \cite{consensus_basics_2} показано, что линейная система
\begin{equation} \label{eq:system_dynamic}
\dot x_i(t) = \sum_{j \in N_i} a_{ij}(x_j(t)-x_i(t))
\end{equation}
представляет собой алгоритм распределенного консенсуса, т.е. гарантирует сходимость к коллективному решению через локальные межагентные взаимодействия. Предполагая, что граф неориентированный ($a_{ij}=a_{ji}, \forall i,j \in V$), следует, что сумма состояний всех узлов является инвариантной величиной, или $\sum_i \dot x_i = 0$. В частности, применив это условие для $t = 0 \text{ и } t = \infty$ получаем:
\begin{equation}
\alpha = \frac{1}{n}\sum_i x_i.
\end{equation}

Другими словами, если консенсус достигается асимптотически, то коллективное решение обязательно равно среднему значению начального состояния всех узлов.

В компактном виде изменения системы (\ref{eq:system_dynamic}) можно записать как
\begin{equation}
\dot x = -Lx
\end{equation}
где $L$ -- лапласиан графа $G$. По определению, $L$ имеет правый собственный вектор, равный $\mathds{1}$, связанный с нулевым собственным значением из-за тождества $L\mathds{1} = 0$. В случае неориентированных графов лапласиан графа удовлетворяет следующему свойству суммы квадратов (ССК):
\begin{equation} \label{eq:sos}
x^TLx = \frac{1}{2}\sum_{(i, j) \in E} a_{ij}(x_j-x_i)^2.
\end{equation}

Определив квадратичную функцию несогласия как
\begin{equation}
\varphi(x) = \frac{1}{2}x^TLx
\end{equation}
становится ясно, что алгоритм (\ref{eq:system_dynamic}) такой же, как
\begin{equation}
\dot x = -\nabla \varphi(x)
\end{equation}
или алгоритм градиентного спуска. Этот алгоритм асимптотически сходится при соблюдении двух условий:
\begin{enumerate}
    \item $L$ - положительно полуопределенная матрица;
    \item единственное решение системы (\ref{eq:system_dynamic}) это $\alpha \mathds{1}$ для некоторого $\alpha$.
\end{enumerate}
Оба эти условия выполняются для связного графа и следуют из ССК-свойства лапласиана графа в (\ref{eq:sos}). Следовательно, средний консенсус достигается асимптотически для всех начальных состояний. Этот факт резюмируется в \cite{consensus_basics} следующей леммой:

\textit{Лемма 1}: Пусть $G$ - связный неориентированный граф. Тогда алгоритм (\ref{eq:system_dynamic}) асимптотически решает задачу среднего консенсуса для всех начальных состояний.

\subsection{Быстрая сходимость}
Выяснив, что алгоритм (\ref{eq:system_dynamic}) асимптотически решает задачу среднего консенсуса, можно задаться вопросом о скорости его сходимости. В \cite{fast_averaging} говорится о том, что можно выбрать такие веса $a_{ij}$ ребер графа $G$, чтобы алгоритм сходился с максимальной скоростью. Иногда некоторые оптимальные веса могут быть даже отрицательными, такой случай рассматривается далее в параграфе \ref{section:main_experiments}. Для нахождения оптимальных весов необходимо решить задачу оптимизации, в которой максимизируется второе из минимальных собственных значений лапласиана $L$ графа $G$ при известных ограничениях на веса ребер. Стоит отметить, что второе собственное значение лапласиана графа возникает из теории графов и называется алгебраическая связность.


\subsection{Консенсус и задача МО}
Применим алгоритм среднего консенсуса к задаче машинного обучения. Пусть каждый вычислительный узел идентифицирован своим индексом $t = \{1, \ldots, p \}$ и хранит свою копию параметров $\theta^t$. Происходит разделение исходных данных $S = \cup_{t=1}^p S^t$, причем $S_i \cup S^j = \oslash \text{ для } i \ne j$. Каждое $S^t$ является множеством исходных данных соответствующего узла $t$. Тогда, в соответствии с \cite{decentralized_sgd}, каждый шаг алгоритма для каждого агента $t$ будет выглядеть как:
\begin{itemize}
\item
    Обучить текущие веса $\theta_k^t$ на подмножестве локальных данных:
     \begin{equation} \label{eq:algo_step_1}
    \theta_{k+\frac{1}{2}}^t =
    \theta_k^t - \alpha_k\frac{1}{|S_k^t|}\sum_{i\in S_k^t}\nabla g(f(x_i, \theta_k), y_i)
    \end{equation}

\item
    Сделать консенсусное усреднение параметров с соседями:
     \begin{equation} \label{eq:algo_step_2}
     \theta_{k+1}^t =
     \sum_{i \in N_t}a_{ti}\theta_{k+\frac{1}{2}}^i
     \end{equation}
где $N_t$ -- соседи узла $t$, причем $t \in N_t$.
\end{itemize}

Шаг (\ref{eq:algo_step_1}) отличается от стандартного подхода (\ref{eq:mbgd}) только тем, что множество $S_k^t$ зависит от $t$, эти множества не пересекаются для разных $t$, то есть происходит разделение данных между вычислительными узлами.

Шаг (\ref{eq:algo_step_2}) –- усреднение параметров модели  между соседними узлами. Здесь происходит обмен информацией между узлами, причем этот шаг не обязан выполняться каждую итерацию.

Как следует из теории, изложенной в предыдущем параграфе, эти шаги эквивалентны обучению одиночной модели с использованием батча $S_k^1 \cup S_k^2 \cup \ldots \cup S_k^p$.

В случае распределенного обучения нейронных сетей под достижением консенсуса мы подразумеваем асимптотическую сходимость параметров моделей на каждом узле $\theta_1^*=\theta_2^*=\ldots=\theta_p^*$.
Однако на практике удобно не отслеживать факт достижения консенсуса с какой-то точностью, а делать конечное количество итераций, заданное вначале.

\subsection{Алгоритм сплетен}
Кроме алгоритма среднего консенсуса к задаче машинного обучения можно применить распределенные алгоритмы сплетен \cite{gossip}. Это класс алгоритмов для обмена информацией и вычислений в произвольно связанной сети узлов. Их основным отличием от алгоритма среднего консенсуса является то, что на конкретной итерации каждый узел может обменяться информацией со случайным соседом в сети. Данный факт означает, что матрица $A$ связей графа динамически меняется во времени, что учитывалось в (\ref{eq:system_dynamic}), а значит на алгоритм сплетен распространяется уже описанная в параграфе \ref{section:algo_mean_cons} теория. На практике консенсусное усреднение отличается от (\ref{eq:algo_step_2}) дополнительным шагом -- выбором случайных пар вершин графа для усреднения:
\begin{itemize}
\item
    Выбрать случайные пары вершин для усреднения:
    \begin{equation} \label{eq:algo_step_3}
    pairs = kRandomElementsFrom(E),
    \end{equation}
где $E$ -- множество ребер графа распределенной сети.

\item
    Сделать консенсусное усреднение параметров с соседями:
     \begin{equation} \label{eq:algo_step_4}
     \theta_{k+1}^i = \theta_{k+1}^j =
     \frac{\theta_{k+\frac{1}{2}}^i + \theta_{k+\frac{1}{2}}^j}{2}, \forall (i, j) \in pairs.
     \end{equation}
\end{itemize}

Усреднения между узлами могут происходить асинхронно, что делает алгоритм сплетен перспективным для применения в слабосвязанных и непостоянных сетях. А количество шагов на этапе усреднения весов может быть выбрано пользователем или автоматически для сохранения высокой скорости сходимости при небольшой нагрузке на коммуникационную сеть. Например, это актуально для мобильных сетей.


\pagebreak
\section{Обзор}
Подходы к параллельным вычислениям делятся на два основных направления:
\begin{itemize}
\item параллелизм по данным (Data Parallel)
\item модельный параллелизм (Model Parallel)
\end{itemize}

В параллелизме по данным происходит вычисление искомой функции каждым узлом на своей части данных. В модельном параллелизме каждый узел вычисляет на общих данных какую-то часть функции. Выбранный для экспериментов подход является разновидностью Data Parallel.

Наиболее часто используемые распределенные системы машинного обучения делятся на синхронные / асинхронные и на централизованные / децентрализованные. На практике их применяют для распределенного вычисления суммы
\begin{equation}
\sum_{i\in S_k}\nabla g(f(x_i, \theta_k), y_i)
\end{equation}
в \ref{eq:mbgd}. Кратко опишу основные существующие подходы:

При синхронном параллельном стохастическом градиентном спуске (S-PSGD) \cite{o1} каждый узел хранит локальную копию основной модели. На каждой итерации он получает минибатч данных от центрального сервера, вычисляет градиент и передает его обратно серверу. После того как центральный сервер получил данные со всех узлов, он вычисляет среднее значение градиента и дает задание каждому узлу обновить веса модели в соответствии с этим значением. Затем наступает следующая итерация.

В асинхронном параллельном стохастическом градиентном спуске (A-PSGD) \cite{o2, o3, o4, o5} тот же подход, но асинхронность достигается за счет дополнительного разрешения узлам использовать устаревшие значения весов при вычислении градиентов. Это позволяет избавиться от необходимости ждать самый медленный узел на каждом шаге синхронизации параметров.

AllReduce стохастический градиентный спуск (AllReduce-SGD) \cite{o6, o7} похож на  S-PSGD,  однако в нем нет сервера параметров, а узлы образуют кольцевую сеть. На очередной итерации каждый узел вычисляет градиент по минибатчу исходных данных. После чего данные передаются по сети, используя парадигму AllReduce, пока каждый узел не получит значения градиентов со всех остальных узлов. Градиенты усредняются и каждый узел обновляет веса своей локальной модели. Из-за наличия фазы синхронизации и большого количества взаимодействий между узлами, данный метод плохо себя показывает в сетях с низкой пропускной способностью.

В децентрализованном параллельном стохастическом градиентном спуске (D-PSGD) \cite{o8, o9, o10} все узлы связаны в сеть, которую можно представить в виде связного графа. Каждый узел имеет свою локальную копию модели. На каждой итерации все узлы вычисляют градиенты по локальным данным и усредняют их, используя градиенты своих соседей в сети. Используя полученное значение градиента, узел обновляет веса своей локальной модели. В данном методе обмен данными и обучение могут происходить параллельно, но из-за синхронизации все равно существует зависимость сети от самого медленного узла.

В 2018 году исследователи из IBM предложили \cite{o11} асинхронный децентрализованный стохастический градиентный спуск (AD-PSGD), основанный на сплетнях. Он отличается от D-PSGD тем, что в каждом узле усредняются не градиенты, а веса модели, и не по всем соседям узла, а с одним случайным. В 2020 году группа исследователей \cite{decentralized_sgd} обобщила и дополнила существующие наработки в этой сфере. В частности, они проанализировали ситуацию, когда случайно выбирается не один, а произвольное число соседей узла. Данный новый метод и был взят за основу для исследования.


\pagebreak
\section{Алгоритм}
Для проведения экспериментов был разработан однопоточный прототип сети, который позволяет обучать, в том числе используя GPU, несколько экземпляров нейронных сетей используя алгоритм среднего консенсуса или алгоритм сплетен, а также собирать, сохранять и визуализировать необходимую для последующего анализа статистику.

Для реализации использовался язык python с фреймворком для работы с нейронными сетями pytorch. Стоит отметить, что инициализация весов нейронных сетей в данном фреймворке происходит случайным образом, из-за чего приходится уравнивать их вручную. Если проигнорировать данный факт, то метод становится менее точным, первые эпохи обучения тратятся на компенсацию случайного разброса параметров, а результаты имеют большую дисперсию.
\subsection{Средний консенсус}
Основные этапы алгоритма обучения нейронных сетей с использованием алгоритма среднего консенсуса:

\begin{breakablealgorithm}
  \caption{Средний консенсус}
\begin{algorithmic}
\State Определить топологию сети $W$ и количество узлов $p$.
\State Инициализировать локальные модели $\theta_0^t$ на всех узлах $t \in \{ 1, \ldots, p \}$.
\State Разделить исходные данные по узлам $S = \cup_{t=1}^p S^t$, причем $S^i \cap S^j = \oslash$ для $i \ne j$.
\State Определить расписание изменения коэффициента скорости обучения $\alpha(k)$.
\State Определить функцию ошибки $g(f(x_i,),y_i)$, где $(x_i,y_i) \in S$.
\State Определить количество эпох $n$, итераций в каждой эпохе $m$, общее количество итераций $K=mn$.
\State Определить частоту итераций консенсуса $q$.

 \For{$k \in \{0\ldots K-1\}$}
    \For{$t \in \{0\ldots p-1\}$}
        \State $S_k^t \leftarrow RandomSubset(S^t)$. \Comment{Выбор батча данных}
        \State $\theta_{k+\frac{1}{2}}^t \leftarrow
            \theta_k^t - \alpha_k\frac{1}{|S_k^t|}\sum_{i\in S_k^t}\nabla g(f(x_i, \theta_k), y_i)$ \Comment{Итерация обучения}

        \If {$k=0 \text{ (mod } q$)} \Comment{Итерация консенсуса}
            \State $ \theta_{k+1}^t \leftarrow
                \sum_{j \in N_t}w_{tj}\theta_{k+\frac{1}{2}}^j$
       \Else
            \State $\theta_{k+1}^t \leftarrow
            \theta_{k+\frac{1}{2}}^t.$
        \EndIf
    \EndFor
\EndFor
\end{algorithmic}
\end{breakablealgorithm}



\subsection{Консенсус с несбалансированными данными}
Предыдущий алгоритм был обобщен для имитации поведения агентов с разной вычислительной мощностью. В этом случае подразумевается, что данные распределяются по узлам пропорционально их вычислительной мощности:

\begin{breakablealgorithm}
  \caption{Консенсус с несбалансированными данными}
\begin{algorithmic}
\State Определить топологию сети $W$ и количество узлов $p$.
\State Инициализировать локальные модели $\theta_0^t$ на всех узлах $t \in \{ 1, \ldots, p \}$.
\State Разделить исходные данные по узлам $S = \cup_{t=1}^p S^t$, причем $S^i \cap S^j = \oslash$ для $i \ne j$. Размеры множеств $S^t$ необязательно равны.
\State Определить расписание изменения коэффициента скорости обучения $\alpha(k)$.
\State Определить функцию ошибки $g(f(x_i,),y_i)$, где $(x_i,y_i) \in S$.
\State Определить количество эпох $n$, итераций в каждой эпохе $m$, общее количество итераций $K=mn$.
\State Определить частоту обучения на одном батче для каждого агента $qt^t$.
\State Определить частоту итераций консенсуса для каждого агента $qc^t$.

 \For{$k \in \{0\ldots K-1\}$}
    \For{$t \in \{0\ldots p-1\}$}
        \State $S_k^t \leftarrow RandomSubset(S^t)$. \Comment{Выбор батча данных}

        \If {$k=0 \text{ (mod } qt^t$)} \Comment{Итерация обучения}
            \State $\theta_{k+\frac{1}{2}}^t \leftarrow
                \theta_k^t - \alpha_k\frac{1}{|S_k^t|}\sum_{i\in S_k^t}\nabla g(f(x_i, \theta_k), y_i)$
        \Else
            \State $\theta_{k+\frac{1}{2}}^t \leftarrow
            \theta_k^t.$
        \EndIf

        \If {$k=0 \text{ (mod } qc^t$)}\Comment{Итерация консенсуса}
            \State $ \theta_{k+1}^t \leftarrow
                \sum_{j \in N_t}w_{tj}\theta_{k+\frac{1}{2}}^j$
       \Else
            \State $\theta_{k+1}^t \leftarrow
            \theta_{k+\frac{1}{2}}^t.$
        \EndIf
    \EndFor
\EndFor
\end{algorithmic}
\end{breakablealgorithm}

\subsection{Сплетни}
Для тестирования работоспособности метода использовалась однопоточная реализация алгоритма. Однако при применении данного метода для обучения нейронных сетей на конечных устройствах, шаг усреднения весов моделей может быть выполнен асинхронно.
\begin{breakablealgorithm}
  \caption{Сплетни}
\begin{algorithmic}
\State Определить топологию сети $W$ и количество узлов $p$.
\State Инициализировать локальные модели $\theta_0^t$ на всех узлах $t \in \{ 1, \ldots, p \}$.
\State Разделить исходные данные по узлам $S = \cup_{t=1}^p S^t$, причем $S^i \cap S^j = \oslash$ для $i \ne j$.
\State Определить расписание изменения коэффициента скорости обучения $\alpha(k)$.
\State Определить функцию ошибки $g(f(x_i,),y_i)$, где $(x_i,y_i) \in S$.
\State Определить количество эпох $n$, итераций в каждой эпохе $m$, общее количество итераций $K=mn$.
\State Определить частоту итераций консенсуса $q$.
\State Определить количество шагов усреднения $b$ на каждой итерации консенсуса.

 \For{$k \in \{0\ldots K-1\}$}
    \For{$t \in \{0\ldots p-1\}$}
        \State $S_k^t \leftarrow RandomSubset(S^t)$. \Comment{Выбор батча данных}
        \State $\theta_{k+1}^t \leftarrow
            \theta_k^t - \alpha_k\frac{1}{|S_k^t|}\sum_{i\in S_k^t}\nabla g(f(x_i, \theta_k), y_i)$ \Comment{Итерация обучения}
    \EndFor
    \If {$k=0 \text{ (mod } q$)} \Comment{Итерация консенсуса}
        \State $pairs \leftarrow RandomChoices(E, b)$ \Comment{Выбор $b$ случайных пар вершин}
        \For{(i, j) $\in$ pairs}
            \State $\theta_{k+1}^i \leftarrow \theta_{k+1}^j \leftarrow \frac{1}{2}(\theta_{k+1}^i + \theta_{k+1}^j)$
        \EndFor
    \EndIf
\EndFor
\end{algorithmic}
\end{breakablealgorithm}


\pagebreak
\section{Эксперименты}
\subsection{Выбор модели и датасета}
В первую очередь стояла задача проводить эксперименты на известном датасете, чтобы иметь возможность отталкиваться от чужих результатов. Была выбрана область классификации изображений и набор данных \flqq CIFAR-10\frqq. Он входит в топ популярнейших датасетов, а также не требует огромных вычислительных ресурсов. Проведя анализ топ state of the art моделей машинного обучения в этой области, пришел к выводу, что большинство решений -- это модификации Residual Network (ResNet) моделей. Исходя из этого принял решение, что нужно проводить эксперименты на каком-нибудь конкретном представителе, выбрав реализацию исходя из:
\begin{itemize}
    \item скорости обучения
    \item количества параметров
    \item наличия реализации на фреймворке с необходимой гибкостью и доступом к параметрам для их модификации
\end{itemize}

В итоге была отобрана реализация ResNet-20 \cite{github_resnet}.

После того, как исследуемые методы неплохо сработали для упомянутой пары модели и датасета, встала задача выбора следующих бенчмарков. Исходя из тех же соображений, выбор пал на датасеты CIFAR-100 и FashionMNIST. Для CIFAR-100 применялась та же модель, что и для CIFAR-10. Для FashionMNIST была написана собвственная реализация Convolutional Neural Network (CNN). Общую информацию о датасетах и моделях можно наблюдать в таблице [ссыль]:

Таблица. Названия датасетов, размера трейна и теста, количество классов. Модели, количество оптимизируемых параметров. Какие модели к каким датасетам применялись.

\subsection{Преобразование данных}
Для воспроизводимости результатов считаю необходимым зафиксировать преобразования, которые осуществлялись над данными перед началом обучения.
Использовались только распространенные в области классификации изображений преобразования:
\begin{itemize}
    \item CIFAR-10 и CIFAR-100
    \begin{itemize}
    \item Поворот изображения по горизонтали с вероятностью 0,5
    \item Случайное обрезание до размера 32 на 32 пикселей с отступом 4 пикселя от края
    \item Нормирование по всем каналам (R, G, B), с параметрами:
        \begin{itemize}
            \item среднее: (0.485, 0.456, 0.406)
            \item среднеквадратическое отклонение: (0.229, 0.224, 0.225)
        \end{itemize}
    \end{itemize}

    \item FashionMNIST
    \begin{itemize}
    \item Нормирование с параметрами:
        \begin{itemize}
            \item среднее: 0.28604
            \item среднеквадратическое отклонение: 0.35302
        \end{itemize}
    \end{itemize}

\end{itemize}

\subsection{Разминка модели}
Исследователи из Facebook, применяя \cite{012} схожий метод к другой моделе и датасету, получили, что предварительная разминка весов моделей перед основным обучением помогает достигнуть лучшего качества и делает процесс обучения более стабильным. Разминка часто успешно применяется при обучении одиночных моделей. Идея заключается в том, чтобы уменьшить скорость обучения на первых эпохах. Действительно, эксперименты показали, что данный подход имеет место быть и при распределенном обучении нескольких моделей с использованием алгоритма среднего консенсуса. Применив постепенную разминку, а именно линейное увеличение скорости обучения до значения стартовой каждую итерацию на протяжении первых пяти эпох, удалось получить прирост точности в среднем на 0.4+-0.05\%.


\subsection{Увеличение скорости обучения}
Исторически в машинном обучении выработалось \cite{lr_batch} общее правило: при увеличении размера батча следует увеличивать скорость обучения. Интуитивно ясно, что чем более точно мы способны посчитать направление локального минимума, тем с большим шагом следует двигаться в эту сторону. Как уже было сказано, применение исследуемых методов эквивалентно обучению одиночной модели с увеличенным батчем, размером в совокупный размер батчей всех узлов сети. Пусть в сети $n$ узлов. В этом случае можно уменьшить локальный размер батча в $n$ раз или применить упомянутое правило и увеличить скорость обучения в $n$ раз. Второй вариант предпочтительнее, ведь уменьшение размера батча повлияет на скорость обучения модели. Однако экспериментальным путем было выяснено, что данный подход следует применять только при малых $n$. Сильное увеличение скорости обучения влечет за собой нестабильный процесс обучения. Возможно, увеличение времени обучения и дополнительный подбор правил уменьшения скорости обучения могут исправить ситуацию, но это серьезное расхождение с оригинальным рецептом обучения, к которому применялись методы, и поэтому не рассматривается.

Успешные и неудачные результаты применения правила увеличения скорости обучения можно наблюдать в Таблице \ref{table:lsr}:


\begin{center}
    \begin{longtable}{|p{3cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{3cm}|}
    \caption{Результаты применения LSR.}
    \label{table:lsr}\\
    \hline
    \textbf{Топология} & \textbf{Датасет} & \textbf{Метод} & \textbf{Размер батча} & \textbf{Точность агентов (\%)} & \textbf{Точность одиночной модели (\%)} \\
    \hline
    топология & датасет & метод & батч & точность агентов & точность одиночной модели\\
    \hline
    \end{longtable}
\end{center}

\subsection{Топология сети}
\label{section:topology}
Как упоминалось в теории, скорость сходимости консенсуса зависит от алгебраической связности графа. Чем выше связность, тем выше скорость. Следовательно, если есть возможность, стоит выбирать топологию сети с оглядкой на этот факт. В том числе, высокой алгебраической связностью обладают графы:
\begin{itemize}
    \item с маленьким диаметром
    \item c высокой реберной связностью
\end{itemize}

На примере экспериментов с 50 агентами, образующими циклы с разным количеством хорд (Рис. \ref{fig:cycle50}), можно проиллюстрировать зависимость скорости сходимости консенсуса от диаметра графа (Рис. \ref{fig:cycle50agents}).

\begin{figure}[H]
\begin{center}
\scalebox{0.9}{
   \includegraphics{images/cycle50.png}
}
\caption{
\label{fig:cycle50}
    Цикл из 50 агентов с хордами.}
\end{center}
\end{figure}



\begin{figure}[H]
\begin{center}
\scalebox{0.3}{
   \includegraphics{images/cycle50experiment.png}
}
\caption{
\label{fig:cycle50agents}
     Эксперимент над циклом из 50 агентов с разным количеством хорд.}
\end{center}
\end{figure}

Существует особый тип графов, называемых экспандерами. Они обладают высокой реберной связностью и хорошо подходят для данной задачи. Процесс обучения 25 агентов, образующих экспандер (Рис. \ref{fig:expander25}), с разной частотой итерации консенсуса, представлен на (Рис. \ref{fig:expander25experiment}).

\begin{figure}[H]
\begin{center}
\scalebox{0.6}{
   \includegraphics{images/expander25.png}
}
\caption{ \label{fig:expander25}
     Экспандер из 25 агентов.}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\scalebox{0.26}{
   \includegraphics{images/expander25experiment.png}
}
\caption{ \label{fig:expander25experiment}
     Эксперимент над экспандером из 25 агентов с разной частотой консенсуса.}
\end{center}
\end{figure}


\subsection{Основные эксперименты}
\label{section:main_experiments}

Кроме перечисленных в предыдущем параграфе топологий сети, эксперименты проводились на цикле из 4 агентов (Рис. \ref{fig:cycle4}), торе $6\times 6$ (Рис. \ref{fig:torus6x6}), полной топологии размера 36 (Рис. \ref{fig:full36}) и случайном графе, для которого среди оптимальных весов, найденных с помощью упомянутой теории для быстрой сходимости, присутствуют отрицательные числа (Рис. \ref{fig:negative_weights}).
Ниже представлены сводные таблицы (Таб. \ref{table:tab1}, \ref{table:tab2}) и графики (Рис. \ref{fig:negative_weights12_experiment}--\ref{fig:torus6x6_unbalanced_b6}), полученные в процессе проведения экспериментов. В том числе на несбалансированных данных с имитацией разной мощности агентов.

\begin{figure}[H]
\begin{center}
\scalebox{0.8}{
   \includegraphics{images/cycle4.png}
}
\caption{ \label{fig:cycle4}
     Цикл из 4 агентов.}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\scalebox{1.0}{
   \includegraphics{images/torus36.pdf}
}
\caption{ \label{fig:torus6x6}
     Тор $6\times 6$.}
\end{center}
\end{figure}


\begin{figure}[H]
\begin{center}
\scalebox{0.8}{
   \includegraphics{images/full36.png}
}
\caption{ \label{fig:full36}
     Полный граф с 36 вершинами.}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\scalebox{0.8}{
   \includegraphics{images/negative_weights.png}
}
\caption{ \label{fig:negative_weights}
     Граф с отрицательными весами.}
\end{center}
\end{figure}


\begin{center}
    \begin{longtable}{|p{5cm}|p{2cm}|p{3cm}|p{3cm}|p{2cm}|}
    \caption{Средний консенсус на сбалансированных данных. Датасет CIFAR-10.}
    \label{table:tab1}\\
    \hline
    \textbf{Топология} & \textbf{Размер батча у агента} & \textbf{Точность агентов (\%)} & \textbf{Точность одиночной модели (\%)} & \textbf{Размер батча одиночной модели}\\
    \hline
    Цикл из 4 агентов & 32 & 92.13+-0.04 & 00.00 & 32*4\\
    \hline
    Экспандер из 25 агентов & 32 & 90.12+-0.07 & 89.30 & 32*25\\
    \hline
    Экспандер из 25 агентов, консенсус реже в 5 раз & 32 & 89.89+-0.10 & 89.30 & 32*25\\
    \hline
    Экспандер из 25 агентов, консенсус реже в 10 раз & 32 & 89.36+-0.11 & 89.30 & 32*25\\
    \hline
    Граф из 50 агентов со 100 ребрами & 32 & 88.40+-0.12 & 88.18 & 32*50\\
    \hline
    Граф из 50 агентов с 150 ребрами & 32 & 88.51+-0.10 & 88.18 & 32*50\\
    \hline
    Граф из 50 агентов со 250 ребрами & 32 & 88.61+-0.08 & 88.18 & 32*50\\
    \hline
    Тор $6\times 6$ & 32 & 89.7 +-0.08 & 89.21 & 32*36\\
    \hline
    Тор $6\times 6$, консенсус реже в 5 раз & 32 & 88.630 +-0.08 & 89.21 & 32*36\\
    \hline
    Тор $6\times 6$ & 6 & 91.55+-0.08 & 91.12 & 6*36\\
    \hline
    Тор $6\times 6$, консенсус реже в 5 раз & 6 & 88.99+-0.08 & 91.12 & 6*36\\
    \hline
    Граф из 12 агентов с отрицательными весами & 16 & 91.80+-0.03 & 91.24 & 16*12\\
    \hline
    \end{longtable}
\end{center}

\begin{center}
    \begin{longtable}{|p{5cm}|p{2cm}|p{3cm}|p{3cm}|p{2cm}|}
    \caption{Средний консенсус на несбалансированных данных. Датасет CIFAR-10.}
    \label{table:tab2}\\
    \hline
    \textbf{Топология} & \textbf{Размер батча у агента} & \textbf{Точность агентов (\%)} & \textbf{Точность одиночной модели (\%)} & \textbf{Размер батча одиночной модели}\\
    \hline
    Цикл из 4 агентов & 32 & 92.19+-0.02 & 00.00 & 32*4\\
    \hline
    Тор $6\times 6$ & 32 & 89.10+-0.14 & 89.21 & 32*36\\
    \hline
    Тор $6\times 6$ & 6 & 90.87+-0.10 & 91.12 & 6*36\\
    \hline
    \end{longtable}
\end{center}


\begin{center}
    \begin{longtable}{|p{5cm}|p{2cm}|p{3cm}|p{3cm}|p{2cm}|}
    \caption{Средний консенсус на сбалансированных данных. Датасет CIFAR-100.}
    \label{table:cifar100}\\
    \hline
    \textbf{Топология} & \textbf{Размер батча у агента} & \textbf{Точность агентов (\%)} & \textbf{Точность одиночной модели (\%)} & \textbf{Размер батча одиночной модели}\\
    \hline
    Цикл из 4 агентов & 32 & 92.19+-0.02 & 00.00 & 32*4\\
    \hline
    \end{longtable}
\end{center}

\begin{center}
    \begin{longtable}{|p{5cm}|p{2cm}|p{3cm}|p{3cm}|p{2cm}|}
    \caption{Средний консенсус на сбалансированных данных. Датасет FashionMNIST.}
    \label{table:mnist}\\
    \hline
    \textbf{Топология} & \textbf{Размер батча у агента} & \textbf{Точность агентов (\%)} & \textbf{Точность одиночной модели (\%)} & \textbf{Размер батча одиночной модели}\\
    \hline
    Цикл из 4 агентов & 32 & 92.19+-0.02 & 00.00 & 32*4\\
    \hline
    \end{longtable}
\end{center}


\begin{center}
    \begin{longtable}{|p{3cm}|p{2cm}|p{2cm}|p{3cm}|p{3cm}|p{2cm}|}
    \caption{Алгоритм сплетен.}
    \label{table:gossip}\\
    \hline
    \textbf{Топология} & \textbf{Датасет} & \textbf{Размер батча у агента} & \textbf{Точность агентов (\%)} & \textbf{Точность одиночной модели (\%)} & \textbf{Размер батча одиночной модели}\\
    \hline
    Цикл из 4 агентов & 32 & 92.19+-0.02 & 00.00 & 32*4\\
    \hline
    \end{longtable}
\end{center}


\begin{figure}[H]
\begin{center}
\scalebox{0.355}{
   \includegraphics{images/negative_weights12experiment.png}
}
\caption{ \label{fig:negative_weights12_experiment}
     Результаты эксперимента на графе с отрицательными весами. Датасет CIFAR-10.}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\scalebox{0.355}{
   \includegraphics{images/torus6x6_b32.png}
}
\caption{ \label{fig:torus6x6_b32}
     Результаты эксперимента на торе $6\times 6$ с батчем 32.}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\scalebox{0.355}{
   \includegraphics{images/torus6x6_b6_different_freqs.png}
}
\caption{ \label{fig:torus6x6_b6_different_freqs}
     Результаты эксперимента на торе $6\times 6$ с батчем 6 и разной частотой консенсуса.}
\end{center}
\end{figure}


\begin{figure}[H]
\begin{center}
\scalebox{0.355}{
   \includegraphics{images/torus6x6_unbalanced_b6.png}
}
\caption{ \label{fig:torus6x6_unbalanced_b6}
     Результаты эксперимента на торе $6\times 6$ с батчем 6 и несбалансированными данными.}
\end{center}
\end{figure}

\pagebreak
\specialsection{Выводы}
Из результатов эксериментов можно сделать несколько выводов. Становится ясно, что алгоритм среднего консенсуса может быть применен для распределенного обучения нейронных сетей. При условии сбалансированности данных во всех экспериментах совместная модель агентов сети достигает той же или даже большей точности, чем одиночная модель с увеличенным батчем. При этом остается возможность проводить раунды консенсуса реже, немного проигрывая в качестве, но уменьшая нагрузку на коммуникационную сеть. Также стало понятно, что описанный алгоритм способен корректно работать в графах с малым количеством ребер, что является важным свойством, ведь на практике каналы связи являются узким местом распределенной сети. Если данные несбалансированы, а мощность агентов пропорциональна размеру их локальных данных, то финальная точность совместной модели получается чуть хуже, чем в сбалансированном случае. Но данное отличие сравнительно невелико, оно меньше заявленного разброса исходной одиночной модели. Вдобавок, как выяснилось, увеличив количество итераций консенсуса, можно достигнуть результата более близкого к результату агентов на сбалансировнных данных. Помимо всего перечисленного, данный алгоритм основан на том, что локальные данные агентов не пересекаются и не передаются по сети в процессе обучения. Это гарантирует изоляцию данных и расширяет границы применимости рассмотренного метода.

Таким образом, алгоритм среднего консенсуса успешно применен к задаче распределенного обучения нейронной сети на заявленном датасете.

\pagebreak
\specialsection{Заключение}
В ходе данной работы исследована возможность применения алгоритма на базе консенсуса для распределенного обучения нейронных сетей, в частности:

\begin{itemize}
  \item изучена предметная область и существующие методы
  \item изучена математическая теория, описывающая исследуемый подход
  \item реализован алгоритм и прототип распределенной сети для проведения экспериментов
  \item проведены эксперименты и выполнен анализ результатов
\end{itemize}
\pagebreak


% Библиография в cpsconf стиле
% Аргумент {1} ниже включает переопределенный стиль с выравниванием слева
\begin{thebibliography}{1}
\bibitem{mbgd} S. Ghadimi, G. Lan, and H. Zhang. \flqq Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization\frqq. Mathematical Programming, 2016.

\bibitem{consensus_basics} R Olfati-Saber, JA Fax, RM Murray. \flqq Consensus and cooperation in networked multi-agent systems\frqq. IEEE, 2007.

\bibitem{consensus_basics_2} R. Olfati-Saber and R. M. Murray, \flqq Consensus problems in networks of agents with switching topology and time-delays\frqq. IEEE Transactions on Automatic Control, vol. 49, no. 9, pp. 1520-1533, Sept. 2004, doi: 10.1109/TAC.2004.834113.

\bibitem{fast_averaging} Boyd S, \flqq Convex optimization of graph Laplacian eigenvalues \frqq. Proceedings of the International Congress of Mathematicians. – 2006. – Т. 3. – №. 1-3. – С. 1311-1319.

\bibitem{decentralized_sgd} Koloskova A. et al. \flqq A unified theory of decentralized SGD with changing topology and local updates\frqq. International Conference on Machine Learning. – PMLR, 2020. – С. 5381-5393.

\bibitem{gossip} Boyd S. et al. \flqq Randomized gossip algorithms //IEEE transactions on information theory\frqq. – 2006. – Т. 52. – №. 6. – С. 2508-2530.

\bibitem{o1} O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. \flqq Optimal distributed online prediction using mini-batches\frqq. Journal of Machine Learning Research, 2012.

\bibitem{o2} A. Agarwal and J. C. Duchi. \flqq Distributed delayed stochastic optimization \frqq. In NIPS, 2011.

\bibitem{o3} H. R. Feyzmahdavian, A. Aytekin, and M. Johansson. \flqq An asynchronous mini-batch algorithm for regularized stochastic optimization\frqq. IEEE Transactions on Automatic Control, 2016.

\bibitem{o4} T. Paine, H. Jin, J. Yang, Z. Lin, and T. Huang. \flqq Gpu asynchronous stochastic gradient descent to speed up neural network training\frqq. arXiv preprint arXiv:1312.6186, 2013.

\bibitem{o5} B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: \flqq A lock-free approach to parallelizing stochastic gradient descent\frqq. In Advances in neural information processing systems, 2011.

\bibitem{o6}  N. Luehr. \flqq Fast multi-gpu collectives with nccl\frqq. Nvidia blog, 2016.

\bibitem{o7}  P. Patarasuk and X. Yuan. \flqq Bandwidth optimal all-reduce algorithms for clusters of workstations\frqq. Journal of Parallel and Distributed Computing, 2009.

\bibitem{o8} Lian X. et al. \flqq Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent\frqq. arXiv preprint arXiv:1705.09056. – 2017.

\bibitem{o9} B. Sirb and X. Ye. \flqq Consensus optimization with delayed and stochastic gradients on decentralized networks\frqq. In Big Data, 2016.

\bibitem{o10} P. Bianchi, G. Fort, and W. Hachem. \flqq Performance of a distributed stochastic approximation algorithm\frqq. IEEE Transactions on Information Theory, 2013.

\bibitem{o11} Xiangru Lian, Wei Zhang, Ce Zhang, Ji Li. \flqq Asynchronous Decentralized Parallel Stochastic Gradient Descent\frqq. Proceedings of the 35th International Conference on Machine Learning, PMLR 80:3043-3052, 2018.

\bibitem{github_resnet} \href{https://github.com/akamaster/pytorch_resnet_cifar10}{GitHub repository with implementing ResNet-20}. // URL: https://github.com/akamaster/pytorch\_resnet\_cifar10

\bibitem{012} Goyal P. et al. \flqq Accurate, large minibatch sgd: Training imagenet in 1 hour\frqq. arXiv preprint arXiv:1706.02677. – 2017.

\bibitem{lr_batch} Smith L. N. \flqq A disciplined approach to neural network hyper-parameters: Part 1--learning rate, batch size, momentum, and weight decay\frqq. //arXiv preprint arXiv:1803.09820. – 2018.

\end{thebibliography}
\end{document}